<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://srikanthbpai.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://srikanthbpai.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-10T16:55:37+00:00</updated><id>https://srikanthbpai.github.io/feed.xml</id><title type="html">blank</title><subtitle>Mathematic </subtitle><entry><title type="html">Predicting Customer Churn Using Logistic Regression</title><link href="https://srikanthbpai.github.io/blog/2025/LogisticRegression/" rel="alternate" type="text/html" title="Predicting Customer Churn Using Logistic Regression"/><published>2025-10-21T13:00:00+00:00</published><updated>2025-10-21T13:00:00+00:00</updated><id>https://srikanthbpai.github.io/blog/2025/LogisticRegression</id><content type="html" xml:base="https://srikanthbpai.github.io/blog/2025/LogisticRegression/"><![CDATA[<p>In today‚Äôs guest post, a B.A. Econ student at MSE, <a href="https://sites.google.com/view/maahika-mathur/">Ms.Maahika Mathur</a> explains the logistic regression problem using the classic <em>Telco Customer Churn</em> dataset. I usually cover this problem in the <em>Probability for GenAI</em> course, co-taught with <a href="https://www.linkedin.com/in/bastyajayshenoy/?originalSubdomain=in">Dr. Ajay Shenoy</a>.<br/> I have a Python notebook that demonstrates both the mathematics and the implementation behind this method. Maahika studied that notebook and offered to write an expository article expanding on it.</p> <div style="text-align:center; margin: 2em 0;"> <iframe src="/assets/pdf/MM_LogisticReg_CustomerChurn.pdf#view=FitH" width="100%" height="800" style="border: 1px solid #ccc; border-radius: 8px;"> </iframe> <p style="font-size: 0.9em; color: #555;"> <em>Maahika Mathur's guest article on Logistic Regression (embedded PDF)</em> </p> </div> <p>In case you‚Äôd like to download and read it at leisure:<br/> <a href="/assets/pdf/MM_LogisticReg_CustomerChurn.pdf">üìÑ Download Maahika‚Äôs illustrated PDF version</a></p>]]></content><author><name>[&quot;Maahika Mathur,&quot;, &quot;Srikanth Pai&quot;]</name></author><category term="professional"/><category term="teaching"/><category term="project"/><category term="logistic-regression"/><category term="customer-churn"/><category term="probability"/><category term="Python"/><summary type="html"><![CDATA[A student guest post on applying logistic regression to telecom churn data.]]></summary></entry><entry><title type="html">Summing up series using Central Limit theorem</title><link href="https://srikanthbpai.github.io/blog/2025/CLTApps/" rel="alternate" type="text/html" title="Summing up series using Central Limit theorem"/><published>2025-10-19T13:00:00+00:00</published><updated>2025-10-19T13:00:00+00:00</updated><id>https://srikanthbpai.github.io/blog/2025/CLTApps</id><content type="html" xml:base="https://srikanthbpai.github.io/blog/2025/CLTApps/"><![CDATA[<p style="text-align:center;"> ü™î ü™î ü™î &nbsp;Happy Deepavali!&nbsp; ü™î ü™î ü™î </p> <p>I wanted to share an unexpected connection that I discovered this morning.<br/> Usually, calculus and analysis are used to prove results about random variables. In this post, we‚Äôll turn that idea on its head ‚Äî we‚Äôll use the Central Limit Theorem to evaluate a purely analytical limit!</p> <p>So I was browsing through <em>Crux Mathematicorum‚Äôs</em> <a href="https://cms.math.ca/wp-content/uploads/2025/10/Wholeissue_51_8-r2.pdf">latest issue</a> and came across this interesting limit problem:</p> <div class="math-block"> <strong>Crux Problem OC750 (pp 377).</strong> <p>Find</p> \[ \lim_{n \to \infty} \sum_{k = n}^{2n} \binom{k - 1}{\,n - 1\,} 2^{-k}. \] </div> <p>Usually <em>Crux</em> does not accept undergraduate-level solutions, so I will discuss one here.</p> <p>It also happens that I am teaching a probability and statistics course right now. One of the fundamental theorems that connects the theory of probability to the practice of statistics is the <em><a href="https://en.wikipedia.org/wiki/Central_limit_theorem">Central Limit Theorem</a></em>. Let me state a simple version of it:</p> <div class="math-block"> <strong>Theorem.</strong> Let \( X_1,X_2,\cdots,X_n \) be a sequence of independent and identically distributed random variables with expectation $\mu$ and variance $\sigma^2$. If \[ S_n := \sum_{i=1}^n X_i, \quad Y_n = \dfrac{S_n - n\mu}{\sqrt{n\sigma^2}}, \] then \[ \lim_{n\to \infty} F_{Y_n}(x) = \int_{-\infty}^{x} \dfrac{e^{-\frac{t^2}{2}}}{\sqrt{2\pi}} \, dt, \] where $F_X$ represents the cdf of $X$. We denote this statement by \(Y_n \xrightarrow{d} N(0,1)\). </div> <hr/> <p>We will solve the Crux problem by identifying the summand as a probability from a classical setup.</p> <div class="math-block"> <strong>Definition.</strong> Let \(S_n\) denote the number of trials required to obtain the \(n\)-th head in a sequence of independent fair coin tosses. Then for \(k \geq n\), \[ \Pr(S_n = k) = \binom{k-1}{\,n-1\,}2^{-k}. \] </div> <p>This distribution is called the <a href="https://en.wikipedia.org/wiki/Negative_binomial_distribution">negative binomial distribution</a> with parameters \((n,1/2)\).</p> <p><em>Proof.</em><br/> Since the \(n\)-th head appears at the \(k\)-th toss, the remaining \(k-1\) tosses must contain exactly \(n-1\) heads.<br/> This chance can be computed using the binomial distribution: \[ \Pr(S_n = k) = \binom{k-1}{\,n-1\,} \left(\frac{1}{2}\right)^{n} \left(\frac{1}{2}\right)^{k-n} = \binom{k-1}{\,n-1\,}2^{-k}. \]</p> <p>Hence our Crux sum can be written as \[ A_n = \sum_{k=n}^{2n} \binom{k-1}{\,n-1\,} 2^{-k} = \Pr(S_n \le 2n), \] that is, the probability that the \(n\)-th head appears by time \(2n\).</p> <p>Now that we have converted the sum to a probability, we can compute the limit using the Central Limit Theorem.<br/> To apply it, we need to express \(S_n\) as a sum of i.i.d. random variables.<br/> Luckily, the negative binomial distribution is exactly such a sum ‚Äî of independent geometric random variables.</p> <hr/> <p>Let‚Äôs recall <a href="https://en.wikipedia.org/wiki/Geometric_distribution">geometric random variables</a></p> <div class="math-block"> <strong>Definition (Geometric random variable).</strong> A random variable \(X\) has the geometric distribution with parameter \(p\) if \[ \Pr(X=k) = (1-p)^{k-1}p, \qquad k = 1,2,\dots \] and it represents the number of trials required for the first success. </div> <p>It has mean \(E[X] = 1/p\) and variance \(\mathrm{Var}(X) = (1-p)/p^2.\) If we define \(X_1, X_2, \dots, X_n\) as i.i.d. geometric\((1/2)\) random variables, then the total number of trials needed to get \(n\) successes is \[ S_n = X_1 + X_2 + \cdots + X_n. \] Note that \(S_n\) is precisely the number of coin tosses required to see exactly \(n\) heads, since \(X_i\) is the interarrival time between the \((i-1)\)th head and the \(i\)th head.</p> <p>Linearity of expectation and variance immediately gives \[ E[S_n] = 2n, \qquad \mathrm{Var}(S_n) = 2n. \]</p> <hr/> <p>Since the random variable \(S_n\) is a <em>sum of i.i.d.</em> geometric\((1/2)\) variables, the Central Limit Theorem implies \[ \frac{S_n - 2n}{\sqrt{2n}} \;\xrightarrow{d}\; N(0,1). \]</p> <p>Thus, for \(Z \sim N(0,1)\), \[ \lim_{n \to \infty}\Pr(S_n \le 2n) = \Pr(Z \le 0) = \tfrac{1}{2}. \]</p> <p>We finally have</p> <div class="math-block"> <p><strong>Conclusion.</strong></p> \[ \lim_{n \to \infty} \sum_{k=n}^{2n} \binom{k-1}{\,n-1\,}2^{-k} = \tfrac{1}{2}. \] </div> <p>So the apparently analytic limit is, in fact, the probability that the \(n\)-th head appears before its expected time.</p> <hr/> <p>As a professor, I will be remiss if I don‚Äôt give my readers a few exercises:</p> <div class="math-block"> <strong>Exercises.</strong> <p>1. Evaluate the following limit:</p> \[ \lim_{n \to \infty} \int_{0}^{n} \frac{x^{n-1} e^{-x}}{(n-1)!}\, dx. \] <p>[Hint: Sum of iid exponentials is the Gamma distribution.]</p> <p>2. Evaluate the following limit:</p> \[ \lim_{n \to \infty} \sum_{k = 0}^{n} e^{-n}\frac{n^k}{k!}. \] <p>[Hint: Sum of iid Poisson random variables is Poisson.]</p> <p>3. Let \(\Gamma(x)\) denote the Gamma function. Evaluate the following limit:</p> \[ \lim_{n \to \infty} \int_{0}^{n} \frac{1}{2^{n/2}\Gamma\!\left(\tfrac{n}{2}\right)} x^{\frac{n}{2}-1} e^{-x/2}\, dx. \] <p>[Hint: Sum of squares of iid standard normals is Chi-square.]</p> <p>4. For a fixed \(p\in(0,1)\), evaluate the limit:</p> \[ \lim_{n\to\infty} \sum_{k=0}^{\lfloor n p\rfloor} \binom{n}{k}\, p^{k}(1-p)^{n - k}. \] <p>[Hint: Sum of iid Bernoulli variables is Binomial.]</p> </div>]]></content><author><name></name></author><category term="Fun"/><category term="CLT"/><category term="Integrals"/><category term="Series"/><summary type="html"><![CDATA[Central limit theorem can be used to compute some series and integrals.]]></summary></entry><entry><title type="html">A measure of risk aversion:Arrow-Pratt Index</title><link href="https://srikanthbpai.github.io/blog/2025/RiskAversion/" rel="alternate" type="text/html" title="A measure of risk aversion:Arrow-Pratt Index"/><published>2025-10-02T13:00:00+00:00</published><updated>2025-10-02T13:00:00+00:00</updated><id>https://srikanthbpai.github.io/blog/2025/RiskAversion</id><content type="html" xml:base="https://srikanthbpai.github.io/blog/2025/RiskAversion/"><![CDATA[<blockquote> <p>‚ÄúUncertainty must be taken in a sense radically distinct from the familiar notion of Risk, from which it has never been properly separated ‚Ä¶ a measurable uncertainty, or ‚Äòrisk‚Äô proper ‚Ä¶ is so far different from an unmeasurable one that it is not in effect an uncertainty at all.‚Äù ‚Äî Frank H. Knight, <em>Risk, Uncertainty and Profit</em> (1921)</p> </blockquote> <p>Everyday life is filled with events whose outcomes cannot be predicted in advance: a medical emergency, a crop failure, a sudden financial crash. Insurance exists to handle such exposure. By paying a premium, households and firms transfer the consequences of an adverse event to an insurer. The premium is the price of protection, and the excess above the actuarially fair value (the expected loss) records how much the individual is willing to pay to avoid volatility.</p> <p>Knight‚Äôs distinction is useful here. Risk refers to situations with well-defined probabilities, as in coin tosses or actuarial tables. Uncertainty refers to situations where probabilities are not even defined. Insurance theory, and the mathematics that follows, lies on the side of risk. A gamble is a random variable with a known distribution, and the central question is: how much will an individual pay to remove it?</p> <p>To study this systematically, we adopt the von Neumann‚ÄìMorgenstern (vNM) framework. An agent is assumed to have a preference ordering over gambles. If these preferences satisfy the vNM axioms, there exists a utility function $u$ such that lotteries are ranked by their expected utility $\mathbb{E}[u(X)]$. Crucially, $u$ is unique only up to a positive affine transformation $v(x)=au(x)+b$. Any meaningful measure of risk aversion must therefore be invariant under such transformations.</p> <p>This immediately rules out naive candidates such as $u‚Äô‚Äô(w)$ or <a href="https://en.wikipedia.org/wiki/Curvature#Graph_of_a_function">geometric curvature</a>, which change under rescaling. The correct object is the Arrow‚ÄìPratt index, \[r(w) = -\frac{u‚Äô‚Äô(w)}{u‚Äô(w)}.\] It is dimensionless, invariant under affine transformations, and arises directly as the leading-order premium per unit variance of a small gamble. In the remainder of this post, we derive this result carefully, work through examples, and reflect on why this ‚Äúeconomic curvature‚Äù has become the standard measure of risk aversion.</p> <hr/> <h3 id="precise-setup">Precise setup</h3> <p>Let start with a simple example to illustrate the ideas that are about to come.</p> <p>Consider a gamble that pays Rs. 2000/- with probability $1/2$ and Rs. 0/- with probability $1/2$. Then $\mathbb{E}[X]=1000$.</p> <ul> <li>A risk-neutral agent demands no premium: $\pi=0$.</li> <li>A risk-averse agent might accept only Rs. 800/- for sure; in this case the agent is willing to forgo Rs. 200/- to avoid uncertainty. We will say that the risk premium is 200 rupees.</li> </ul> <p>Now we get on with the math. Let $(\Omega,\mathcal{F},P)$ be a probability space, and let $X:\Omega\to\mathbb{R}$ be a random payoff. An agent with initial wealth $w$ evaluates outcomes through a utility function $u:\mathbb{R}\to\mathbb{R}$, continuous, strictly increasing, and differentiable.</p> <p>The agent faces two options:</p> <ol> <li><em>Risky option:</em> receive the random payoff $w + X$.</li> <li><em>Certain option:</em> receive the fixed payoff $w + c$ .</li> </ol> <p>Indifference between these two options is expressed by \[ \mathbb{E}[u(w+X)] = u(w+c). \]</p> <p>The value $c$ solving this equation is called the certainty equivalent of the gamble $X$.</p> <p>The premium $\pi$ that an agent pays over the acturial fair value $\mathbb{E}(X)$ is called the <em>risk premium</em>. Mathematically,<br/> \[ \pi = \mathbb{E}(X)-c \]</p> <p>Substituting the certainty equivalent into the definition of the premium yields the fundamental equation characterizing the risk premium. We make this our definition of risk premium following <a class="citation" href="#pratt1964risk">(Pratt, 1964)</a>:</p> <div class="math-block"> <strong>Definition (Risk premium).</strong> The risk premium $\pi$, associated to a gamble \(X\), of an agent with with wealth \(w\) and utility \(u\), is given by the following equation: \[ \mathbb{E}[u(w+X)] \;=\; u\!\big(w+\mathbb{E}[X]-\pi\big). \] </div> <p>We can show that $\pi$ exists and is unique if we make standard assumptions.</p> <div class="math-block"> <strong>Proposition.</strong> If \( u \) is a concave continuous increasing function then for any real $w$ and random variable $X$ there exists a unique positive real number $\pi$ such that \[ \mathbb{E}[u(w+X)] \;=\; u\!\big(w+\mathbb{E}[X]-\pi\big). \] </div> <div class="math-block proof"> <strong>Proof.</strong> By Jensen's inequality for concave functions \(u\), \[\mathbb{E}(u\left(w+X\right)) &lt; u\left(w+\mathbb{E}(X)\right).\] Since the $u$ is continuous, there must exist a real number \(\pi\) so that \[\mathbb{E}(u\left(w+X\right)) = u\left(w+\mathbb{E}(X)-\pi\right).\] Further $\pi$ unique since the function is increasing.‚àé </div> <h4 id="risk-in-the-context-of-insurance">Risk in the context of insurance</h4> <p>In insurance terminology, the <em>loading ratio</em> (or <em>loading factor</em>) is defined as the multiple by which the <em>gross premium</em> exceeds the <em>actuarially fair premium</em> (i.e. expected losses). In other words,<br/> \[ \text{loading ratio} = \frac{\text{gross premium}}{\text{expected loss}}. \]<br/> This captures all extra costs insurers build in ‚Äî administrative expenses, underwriting, profit margins, risk margins, etc.</p> <p>Empirically, loading ratios vary quite a lot across sectors. In health insurance, administrative loadings often add <strong>5% to 20%</strong> above expected claims in competitive markets (<a href="https://link.springer.com/article/10.1007/s10198-022-01436-y">Springer Health Econ, 2022</a>). In specialty or retail markets (like travel insurance, warranty contracts), loadings may be far more extreme, sometimes multiple times the fair value (<a href="https://www.partnerslife.co.nz/news-and-views/what-are-premium-loadings">Partners Life, NZ</a>). In property/casualty lines, loss ratios (i.e. claims √∑ premiums) often lie in the 70‚Äì99% range, leaving the balance for loadings and profit (<a href="https://en.wikipedia.org/wiki/Loss_ratio">Wikipedia: Loss ratio</a>).</p> <p>In India, loading ratios are often significant. For example, health insurers routinely impose <em>loading charges</em> on individuals with pre-existing conditions, age, or unhealthy behavior as an extra amount over the base premium (<a href="https://www.acko.com/health-insurance/loading/">Acko Health Insurance</a>). In the non-life sector, motor insurance sometimes runs at very high combined ratios, with reports that the combined ratio might approach <strong>200 %</strong> in some years (i.e. gross premium double the expected claims + expenses) (<a href="https://www.business-standard.com/article/finance/motor-insurance-combined-ratios-may-touch-200-by-end-of-fy15-114040100041_1.html">Business Standard</a>). In the motor third-party (TP) segment, it is observed that while motorcycle TP contributes 15.5 % of TP premium, its share of incurred TP claims is 21.1 %, indicating a mismatch of premium to claims across classes (<a href="https://www.gicouncil.in/yearbook/2021-22/indian-non-life-insurance-industry-analysis/section-04-segmentwise-business-highlights/motor-third-party-tp/">GIC Council Yearbook 2021-22</a>).</p> <p>Thus in India, it is not unusual for gross premiums to be <em>1.5√ó, 2√ó, or even higher multiples</em> of actuarial expected loss, depending on line and risk class.</p> <p>Now lets a see an example calculation assuming a concave utility function.</p> <p><em>Example:</em></p> <p>Consider an agent with a wealth \(w = 10\) lakh rupees and utility function \(u(c) = \sqrt{c}\). The agent faces a small risk: with probability \(0.01\) a loss of \(10{,}000\) occurs, and with probability \(0.99\) no loss occurs. The actuarially fair premium for this gamble is \(0.01 \times 10{,}000 = 100\).</p> <p>With no insurance, expected utility is<br/> \[ \mathbb{E}[u(w+X)] = 0.99\sqrt{10{,}00{,}000} + 0.01\sqrt{9{,}90{,}000} \approx 999.95. \]</p> <p>With insurance, certain wealth is \(9{,}99{,}900 - \pi\), giving utility<br/> \[ u(9{,}99{,}900 - \pi) = \sqrt{9{,}99{,}900 - \pi}. \]</p> <p>Equating the two expressions,<br/> \[ \sqrt{9{,}99{,}900 - \pi} \approx 999.95, \] which solves to \(\pi \approx 95\).</p> <p>Hence the household would pay about ‚Çπ195 in total (‚Çπ100 fair value plus a ‚Çπ95 risk premium) to eliminate the 1% risk of losing ‚Çπ10,000. In this example, the ratio of willingness-to-pay to the actuarially fair premium is<br/> \[ \frac{195}{100} = 1.95. \]<br/> In this example, the household‚Äôs willingness-to-pay ratio is 1.95, meaning they would pay nearly double the actuarially fair premium. In a perfectly competitive and frictionless insurance market, such willingness-to-pay would put an upper bound on the loading ratio insurers could sustain.</p> <p>In the previous example, the square root utility function was arbitrary. The exact value of the risk premium is certainly sensitive to the utility function used. Thus if we are interested in computing premiums, we are forced to contemplate the design of utility functions.</p> <p>In order to understand such a design, we look at a measure of local risk aversion. The pertinent question now is: <em>how much premium will the agent pay if the gamble is small compared to their wealth?</em></p> <p>We talk about small gambles by introducing a scale factor. We scale a gamble by a factor \(t\) and study the risk premium \(\pi(t)\) defined by</p> <p>\[ u(w-\pi(t)) \;=\; \mathbb{E}[u(w+tX)]. \]</p> <hr/> <div class="math-block"> <strong>Proposition (Leading-order risk premium).</strong> Let \(u \in C^{3}\) with \(u'(w) &gt; 0\). Let \(X\) satisfy \(\mathbb{E}[X] = 0\) and \(\mathrm{Var}(X) = \sigma^{2} &lt; \infty\). Define \(\pi(t)\) by \[ u(w-\pi(t)) = \mathbb{E}[u(w+tX)]. \] Then, as \(t \to 0\), \[ \pi(t) \;=\; \tfrac12\,r(w)\,\sigma^{2}\,t^{2} + o(t^{2}), \qquad\text{where}\qquad r(w) = -\,\frac{u''(w)}{u'(w)}. \] </div> <div class="math-block proof"> <strong>Proof.</strong> Taylor-expand the left-hand side at \(w\): \[ u(w-\pi) = u(w) - u'(w)\pi + \tfrac12 u''(w)\pi^{2} + O(\pi^{3}). \] Expand the right-hand side inside the expectation and take expectations: \[ u(w+tX) = u(w) + u'(w)tX + \tfrac12 u''(w)t^{2}X^{2} + O(t^{3}|X|^{3}), \] so that \[ \mathbb{E}[u(w+tX)] = u(w) + \tfrac12 u''(w)\sigma^{2}t^{2} + O(t^{3}\,\mathbb{E}|X|^{3}), \] using \(\mathbb{E}[X] = 0\). Equating both sides and canceling \(u(w)\) gives \[ -\,u'(w)\pi(t) + \tfrac12 u''(w)\pi(t)^{2} + O(\pi(t)^{3}) = \tfrac12 u''(w)\sigma^{2}t^{2} + O(t^{3}). \] Since the right-hand side is \(O(t^{2})\), we must have \(\pi(t) = O(t^{2})\); hence \(\pi(t)^{2} = O(t^{4}) = o(t^{2})\). Neglecting higher-order terms, \[ -\,u'(w)\pi(t) = \tfrac12 u''(w)\sigma^{2}t^{2} + o(t^{2}), \] which simplifies to \[ \pi(t) = \tfrac12\left(-\frac{u''(w)}{u'(w)}\right)\sigma^{2}t^{2} + o(t^{2}) = \tfrac12\,r(w)\,\sigma^{2}t^{2} + o(t^{2}).‚àé \] </div> <p>So the relation between local risk aversion $r(w)$, volatility of the gamble $\sigma^2t^2$ and risk premium $\pi$ is \[ \text{local risk aversion} = \dfrac{2\times \text{ risk premium}}{\text{volatility of the gamble }} \]</p> <p>The definition of $r(w)$ in terms of utility allows us to design utility function by solving odes.</p> <p><em>Example 1:</em> If $r(w)=0$ for all $w$, then ${u‚Äô‚Äô}(w)=0$. Thus $u(w)$ is linear. This is the risk neutral case. Also note that if $u$ is concave, increasing then $r(w) &gt; 0$ which is usually the risk averse case.</p> <p><em>Example 2:</em> Now lets work out the constant risk aversion case. For some real number $c$, we have $r(w) = c$ for all $w$. In this case we can solve the differential equation \[{u‚Äô‚Äô}(w) = -cu‚Äô(w)\] by integrating twice. We get $u(w) = -e^{-cw}/c$ upto scaling. This is a concave utility function which remains equivalent under shifts of wealth. So at any value of wealth, the risk premium must be the same. This is true from the theorem above since r is constant.</p> <p><em>Example 3:</em> A famous example is when $wr(w) = \gamma$. Assuming $\gamma \neq 1, \gamma &gt;0$, the solution to \[w{u‚Äô‚Äô}(w) = -\gamma u‚Äô(w)\] is given by \[u(w) = \dfrac{w^{(1-\gamma)}-1}{1-\gamma}.\] This utility function is called constant relative risk aversion (CRRA) function. There is a way to justify the CRRA name but I will not explain it here. If you meet me on the street, you can ask me :)</p> <p>If utility is rescaled as $\tilde u = a u + b$ with $a&gt;0$, then \[ -\,\frac{\tilde u‚Äô‚Äô}{\tilde u‚Äô} = -\,\frac{u‚Äô‚Äô}{u‚Äô}, \] so the Arrow‚ÄìPratt index is unchanged. The measure of risk aversion depends only on preferences, not units. This property ensures that the measure of risk aversion does not depend on arbitrary rescalings or shifts of utility, but only on the underlying preference structure. In other words, $r(w)$ captures a genuine feature of risk preferences rather than a mere artifact of normalization.</p> <p>We began with the everyday intuition of paying insurance premiums to reduce uncertainty. By formalizing this as a utility maximization problem, we derived the <strong>risk premium</strong> $\pi$ as the key measure of aversion to risk. For small gambles, the <strong>Arrow‚ÄìPratt index</strong> $r(w)$ precisely quantifies the willingness to pay per unit of variance.</p> <hr/> <h2 class="bibliography">1964</h2> <ol class="bibliography"><li><div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="pratt1964risk" class="col-sm-8"> <div class="title">Risk Aversion in the Small and in the Large</div> <div class="author"> John W. Pratt </div> <div class="periodical"> <em>Econometrica</em>, 1964 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> ]]></content><author><name></name></author><category term="professional"/><category term="teaching"/><category term="project"/><category term="utility"/><category term="risk-aversion"/><category term="Arrow-Pratt"/><summary type="html"><![CDATA[On risk premium computations and the resulting measure of local risk aversion .]]></summary></entry><entry><title type="html">Copulas - the art of avoiding marginal distributions!</title><link href="https://srikanthbpai.github.io/blog/2025/Copulas/" rel="alternate" type="text/html" title="Copulas - the art of avoiding marginal distributions!"/><published>2025-05-26T15:59:00+00:00</published><updated>2025-05-26T15:59:00+00:00</updated><id>https://srikanthbpai.github.io/blog/2025/Copulas</id><content type="html" xml:base="https://srikanthbpai.github.io/blog/2025/Copulas/"><![CDATA[<p>The simplest decision problem is forecasting the value of one variable in terms of another. We will focus on the relationship between stock prices and index prices, usually called a market model, in this blogpost. Our job as academics is to tease out the precise meaning of the word ‚Äòrelation‚Äô.</p> <p>Consider an imaginary table of stock price vs index price:</p> <table> <thead> <tr> <th>Stock Price (X)</th> <th>Index Price (Y)</th> </tr> </thead> <tbody> <tr> <td>2</td> <td>45</td> </tr> <tr> <td>2</td> <td>52</td> </tr> <tr> <td>2</td> <td>48</td> </tr> <tr> <td>3</td> <td>52</td> </tr> <tr> <td>3</td> <td>60</td> </tr> <tr> <td>4</td> <td>65</td> </tr> <tr> <td>4</td> <td>68</td> </tr> <tr> <td>4</td> <td>70</td> </tr> <tr> <td>5</td> <td>68</td> </tr> <tr> <td>5</td> <td>75</td> </tr> <tr> <td>5</td> <td>80</td> </tr> <tr> <td>6</td> <td>80</td> </tr> </tbody> </table> <blockquote> <p>How do we find a relation between the two variables?</p> </blockquote> <h3 id="from-functional-relations-to-distributions">From Functional Relations to Distributions</h3> <p>The figure below illustrates three types of relationships we can observe:</p> <ol> <li>Ideally, we would love to get a functional relation $Y=h(X)$, i.e. a unique value of $Y$ for each value of $X$. The economists favorite tool ‚Äòlinear regression‚Äô is popular because who doesn‚Äôt love functional relations. Linear is easy so it sweetens the pot!</li> <li>However, in practice, when we collect data and stare at the table, we usually get multivalued function, i.e. $h(X)$ can be a set of values.</li> <li>When sufficient number of data collection experiments are repeated, the data usually has more information: we can keep track of the multiplicity of values. So we get a ‚Äòdistribution‚Äô!</li> </ol> <figure style="text-align: center;"> <img src="/assets/img/fun2dist.png" alt="Description" style="width: 70%; height: auto;"/> <figcaption style="margin-top: 0.5em; font-size: 0.9em; color: #666;"> <em>Figure: Three forms of relations.</em> </figcaption> </figure> <p>In summary, we may view a conditional distribution $\Pr\{Y=y \mid X=x\}$ as a ‚Äòstatistical function‚Äô. For every input value of $x$, it gives us a distribution of $y$ values instead of a single value. Traditionally, in probability theory, conditional distributions are computed using joint distributions of $(X,Y)$. Information about the marginal distributions is stored within the joint distribution.</p> <h3 id="copulas-capturing-dependence-without-marginals">Copulas: Capturing Dependence Without Marginals</h3> <p>The aim of this blogpost is to motivate an interesting form of ‚Äòjoint distribution‚Äô, called ‚ÄòCopula‚Äô, that forgets the marginal distribution information! Think of it as analogous to a linear relation after converting both the variables to percentages. So its like a relation between normalized variables.</p> <p>This becomes especially useful when there is no consensus on the marginal distributions of the variables involved. A good example is the distribution of daily stock returns. These are often assumed to follow a normal distribution, but in reality, large deviations (extreme returns) occur far too frequently for that assumption to hold. Recognizing this, Mandelbrot (in @mandelbrot1963variation) and Fama (in @fama1965behavior) later proposed using L√©vy-stable distributions as more appropriate alternatives.</p> <blockquote> <p>If distribution of returns is controversial, can we study market models independent of such assumptions?</p> </blockquote> <p>YES! We can use <em>copulas</em>.</p> <p>The idea of copula is motivated by a cute mathematical observation.</p> <div class="math-block"> <strong>Proposition.</strong> Let \( X \) be a random variable with a continuous CDF \( F \) and let \( Y = F(X). \) Then \(Y\) is uniformly distributed on the interval \([0,1].\) </div> <div class="math-block proof"> <strong>Proof.</strong> Since \( F \) is continuous and increasing, it is invertible. By properties of cdf, we know that \(0 &lt; Y &lt; 1.\) Consider a real number \(y\) such that \(0 &lt; y &lt; 1\), let us compute the cdf of \(Y: \) $$ \begin{align} F_Y(y) &amp;= \Pr\{Y \leq y\} \\ &amp;= \Pr\{F(X) \leq y\} \\ &amp;= \Pr\{X \leq F^{-1}(y)\} \\ &amp;= F(F^{-1}(y)) \\ &amp;= y \end{align} $$ Hence, \( Y \sim \text{Uniform}(0,1) \). ‚àé </div> <p>So we have a universal method to transform a random variable with known distribution into a uniform random variable! Now suppose we have two variables $X,Y$ with marginal cdfs $F_X,F_Y$, the joint cdf of the uniform random variables $U_X = F_X(X), U_Y=F_Y(Y)$ is called the <em>copula</em> of $X,Y$ (see @sklar1959).</p> <div class="math-block"> <strong>Definition.</strong> Let $X,Y$ be two random variables with marginal cdfs $F_X,F_Y$. Then the copula $C(u,v)$ is defined as \[ C(u,v) = \Pr\left\{X \leq F_X^{-1}(u),Y \leq F_Y^{-1}(v)\right\},\] for $0 &lt; u,v &lt; 1$. </div> <p>The copula has the following cool features:</p> <ol> <li>The copula is a joint distribution of two uniform random variables. The marginal distribution of $X,Y$ is lost and cannot be recovered from the copula.</li> <li>If $X,Y$ are independent, then copula $C(u,v) = uv$. If they are completely dependent, say $X=Y$, then $C(u,v) = \text{min}\{u,v\}$. Thus copula still tracks the relation between variables even though the marginal information is lost.</li> <li>Sklar‚Äôs theorem allows you to build a joint cdf by assembling a copula and a pair of marginal distributions!</li> </ol> <p>In finance, the controversial question of whether daily returns of stock prices are normally distributed can be side stepped with copulas. We can focus on a copula based market model where we estimate the copula of stock return and index return. The conditional distribution of stock crash given that the market has crashed is called the ‚Äòmarket crash risk‚Äô. <a href="https://www.mse.ac.in/faculty/ekta/">Dr. Selarka</a>, my MA student Amritha, and I estimated the market crash risk for Nifty50 and it forms the main part of Amritha‚Äôs dissertation. Amritha has written python modules for copula estimation following the ideas in Chabi-Yo <em>et. al.</em> and I leave you with pretty pictures from her dissertation. If you are curious, read it and let us know your comments.</p> <figure style="text-align: center;"> <img src="/assets/img/copulas-amritha.png" alt="Description" style="width: 50%; height: auto;"/> <figcaption style="margin-top: 0.5em; font-size: 0.9em; color: #666;"> <em>Figure: If you are curious about these pretty pictures, read Amritha's dissertation. </em> </figcaption> </figure>]]></content><author><name></name></author><category term="professional"/><category term="teaching"/><category term="project"/><category term="probability"/><category term="stocks"/><summary type="html"><![CDATA[Market crash risk without marginal information?]]></summary></entry><entry><title type="html">Andre‚Äôs Reflection Trick</title><link href="https://srikanthbpai.github.io/blog/2024/Andres-reflection-trick/" rel="alternate" type="text/html" title="Andre‚Äôs Reflection Trick"/><published>2024-09-29T15:59:00+00:00</published><updated>2024-09-29T15:59:00+00:00</updated><id>https://srikanthbpai.github.io/blog/2024/Andres-reflection-trick</id><content type="html" xml:base="https://srikanthbpai.github.io/blog/2024/Andres-reflection-trick/"><![CDATA[<p>Here‚Äôs an interesting problem related to voting and ballot counts:</p> <p><strong>Suppose A and B win p and q votes respectively in an election with p &gt; q. While counting the ballots, what is the probability that A always remains ahead of B throughout the count?</strong></p> <p>The problem is recast graphically and then Andre‚Äôs beautiful insight solves the problem quickly. Watch it explained in this video:</p> <iframe style="display: block; margin: auto;" width="560" height="315" src="https://www.youtube.com/embed/ebKPaj9Pj6s?si=2NHw--F9A2G60vwM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>]]></content><author><name></name></author><category term="Cool"/><category term="math"/><category term="Voting"/><category term="problems"/><category term="Probability"/><summary type="html"><![CDATA[Probability problem on vote count.]]></summary></entry></feed>