---
layout: post
date: 2025-10-19 09:00:00-0400
title: Summing up series using Central Limit theorem
description: Central limit theorem can be used to compute some series and integrals.
tags:
  - CLT
  - Integrals
  - Series
related_posts: true
giscus_comments: true
display_categories:
  - teaching
categories: "Fun"
related_publications: false
---

{% capture content %}

<p style="text-align:center;">
ðŸª” ðŸª” ðŸª” &nbsp;Happy Deepavali!&nbsp; ðŸª” ðŸª” ðŸª”
</p>
I wanted to share an unexpected connection that I discovered this morning.  
Usually, calculus and analysis are used to prove results about random variables. In this post, weâ€™ll turn that idea on its head â€” weâ€™ll use the Central Limit Theorem to evaluate a purely analytical limit!

So I was browsing through *Crux Mathematicorumâ€™s* [latest issue](https://cms.math.ca/wp-content/uploads/2025/10/Wholeissue_51_8-r2.pdf) and came across this interesting limit problem:

<div class="math-block">
<strong>Crux Problem OC750 (pp 377).</strong>
<p>Find</p>
\[
\lim_{n \to \infty} 
\sum_{k = n}^{2n} 
\binom{k - 1}{\,n - 1\,} 2^{-k}.
\]
</div>

Usually *Crux* does not accept undergraduate-level solutions, so I will discuss one here.

It also happens that I am teaching a probability and statistics course right now. One of the fundamental theorems that connects the theory of probability to the practice of statistics is the *[Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem)*. Let me state a simple version of it:

<div class="math-block">
<strong>Theorem.</strong> Let \( X_1,X_2,\cdots,X_n \) be a sequence of independent and identically distributed random variables with expectation $\mu$ and variance $\sigma^2$. If 
\[
S_n := \sum_{i=1}^n X_i, \quad Y_n = \dfrac{S_n - n\mu}{\sqrt{n\sigma^2}},
\]
then 
\[
\lim_{n\to \infty} F_{Y_n}(x) = \int_{-\infty}^{x} \dfrac{e^{-\frac{t^2}{2}}}{\sqrt{2\pi}} \, dt.
\]
We denote this statement by \(Y_n \xrightarrow{d} N(0,1)\).
</div>

---

We will solve the Crux problem by identifying the summand as a probability from a classical setup.

<div class="math-block">
<strong>Definition.</strong> Let \(S_n\) denote the number of trials required to obtain the \(n\)-th head in a sequence of independent fair coin tosses. Then for \(k \geq n\),
\[
\Pr(S_n = k) = \binom{k-1}{\,n-1\,}2^{-k}.
\]
</div>

This distribution is called the [negative binomial distribution](https://en.wikipedia.org/wiki/Negative_binomial_distribution) with parameters \\((n,1/2)\\).

*Proof.*  
Since the \\(n\\)-th head appears at the \\(k\\)-th toss, the remaining \\(k-1\\) tosses must contain exactly \\(n-1\\) heads.  
This chance can be computed using the binomial distribution:
\\[
\Pr(S_n = k) = \binom{k-1}{\,n-1\,}
\left(\frac{1}{2}\right)^{n}
\left(\frac{1}{2}\right)^{k-n}
= \binom{k-1}{\,n-1\,}2^{-k}.
\\]

Hence our Crux sum can be written as
\\[
A_n = \sum_{k=n}^{2n} \binom{k-1}{\,n-1\,} 2^{-k} = \Pr(S_n \le 2n),
\\]
that is, the probability that the \\(n\\)-th head appears by time \\(2n\\).

Now that we have converted the sum to a probability, we can compute the limit using the Central Limit Theorem.  
To apply it, we need to express \\(S_n\\) as a sum of i.i.d. random variables.  
Luckily, the negative binomial distribution is exactly such a sum â€” of independent geometric random variables.

---
Let's recall [geometric random variables](https://en.wikipedia.org/wiki/Geometric_distribution)
<div class="math-block">
<strong>Definition (Geometric random variable).</strong>  
A random variable \(X\) has the geometric distribution with parameter \(p\) if
\[
\Pr(X=k) = (1-p)^{k-1}p, \qquad k = 1,2,\dots
\]
and it represents the number of trials required for the first success.
</div>

It has mean \\(E[X] = 1/p\\) and variance \\(\mathrm{Var}(X) = (1-p)/p^2.\\)
If we define \\(X_1, X_2, \dots, X_n\\) as i.i.d. geometric\\((1/2)\\) random variables, then the total number of trials needed to get \\(n\\) successes is
\\[
S_n = X_1 + X_2 + \cdots + X_n.
\\]
Note that \\(S_n\\) is precisely the number of coin tosses required to see exactly \\(n\\) heads, since \\(X_i\\) is the interarrival time between the \\((i-1)\\)th head and the \\(i\\)th head.

Linearity of expectation and variance immediately gives
\\[
E[S_n] = 2n, \qquad \mathrm{Var}(S_n) = 2n.
\\]

---

Since the random variable \\(S_n\\) is a <em>sum of i.i.d.</em> geometric\\((1/2)\\) variables, the Central Limit Theorem implies
\\[
\frac{S_n - 2n}{\sqrt{2n}} \;\xrightarrow{d}\; N(0,1).
\\]

Thus, for \\(Z \sim N(0,1)\\),
\\[
\lim_{n \to \infty}\Pr(S_n \le 2n) = \Pr(Z \le 0) = \tfrac{1}{2}.
\\]

We finally have
<div class="math-block">
<p><strong>Conclusion.</strong></p>
\[
\lim_{n \to \infty}
\sum_{k=n}^{2n}
\binom{k-1}{\,n-1\,}2^{-k}
= \tfrac{1}{2}.
\]
</div>

So the apparently analytic limit is, in fact, the probability that the \\(n\\)-th head appears before its expected time.  

---

As a professor, I will be remiss if I donâ€™t give my readers a few exercises:

<div class="math-block">
<strong>Exercises.</strong>

<p>1. Evaluate the following limit:</p>

\[
\lim_{n \to \infty} 
\int_{0}^{n} 
\frac{x^{n-1} e^{-x}}{(n-1)!}\, dx.
\]
<p>[Hint: Sum of iid exponentials is the Gamma distribution.]</p>

<p>2. Evaluate the following limit:</p>

\[
\lim_{n \to \infty}
\sum_{k = 0}^{n}
e^{-n}\frac{n^k}{k!}.
\]
<p>[Hint: Sum of iid Poisson random variables is Poisson.]</p>

<p>3. Let \(\Gamma(x)\) denote the Gamma function. Evaluate the following limit:</p>

\[
\lim_{n \to \infty}
\int_{0}^{n}
\frac{1}{2^{n/2}\Gamma\!\left(\tfrac{n}{2}\right)}
x^{\frac{n}{2}-1} e^{-x/2}\, dx.
\]
<p>[Hint: Sum of squares of iid standard normals is Chi-square.]</p>

<p>4. For a fixed \(p\in(0,1)\), evaluate the limit:</p>

\[
\lim_{n\to\infty}
\sum_{k=0}^{\lfloor n p\rfloor}
\binom{n}{k}\, p^{k}(1-p)^{n - k}.
\]
<p>[Hint: Sum of iid Bernoulli variables is Binomial.]</p>
</div>



{% endcapture %}
{{ content | markdownify }}